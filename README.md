## Content

#### Popular Optimization algorithms
- SGD [[Book]](https://www.deeplearningbook.org/contents/optimization.html)
- Momentum [[Book]](https://www.deeplearningbook.org/contents/optimization.html)
- RMSProp [[Book]](https://www.deeplearningbook.org/contents/optimization.html)
- AdaGrad [[Link]](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
- ADAM [[Link]](https://arxiv.org/abs/1412.6980)
- AdaBound [[Link]](https://arxiv.org/abs/1902.09843) [[Github]](https://github.com/Luolc/AdaBound)

#### Normalization Methods
- BatchNorm [[Link]](https://arxiv.org/abs/1502.03167)
- Weight Norm [[Link]](http://papers.nips.cc/paper/6113-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks)
- Spectral Norm [[Link]](https://arxiv.org/abs/1802.05957)

#### On Convexity of Neural Networks
- Convex Neural Networks [[Link]](http://papers.nips.cc/paper/2800-convex-neural-networks.pdf)
- Breaking the Curse of Dimensionality with Convex Neural Networks [[Link]](http://jmlr.org/papers/volume18/14-546/14-546.pdf)
- UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION [[Link]](https://arxiv.org/pdf/1611.03530.pdf)
- Optimal Control Via Neural Networks: A Convex Approach. [[Link]](https://openreview.net/forum?id=H1MW72AcK7)
- Input Convex Neural Networks [[Link]](https://arxiv.org/pdf/1609.07152.pdf)
- A New Concept of Convex based Multiple Neural Networks Structure. [[Link](http://www.ifaamas.org/Proceedings/aamas2019/pdfs/p1306.pdf)
- SGD Converges to Global Minimum in Deep Learning via Star-convex Path [[Link]](https://arxiv.org/abs/1901.00451)

#### Solving simpler problem first
- Curriculum Learning [[Link]](https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf)
- Noisy Activation Function [[Link]](http://proceedings.mlr.press/v48/gulcehre16.pdf)
- Mollifying Networks [[Link]](https://arxiv.org/abs/1608.04980)
- Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks [Link](https://arxiv.org/pdf/1802.03796.pdf)
- Parameter Continuation with Secant Approximation for Deep Neural Networks and Step-up GAN [Link](https://digitalcommons.wpi.edu/etd-theses/1256/)
- HashNet: Deep Learning to Hash by Continuation. [[Link]](https://arxiv.org/abs/1702.00758)
- Learning Combinations of Activation Functions. [[Link]](https://arxiv.org/pdf/1801.09403.pdf)
- Learning and development in neural networks: The importance of starting small (1993) [Link](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.128.4487&rep=rep1&type=pdf)
- Flexible shaping: How learning in small steps helps [Link](https://www.sciencedirect.com/science/article/pii/S0010027708002850)

#### On Loss Surfaces of Deep Neural Networks
- QUALITATIVELY CHARACTERIZING NEURAL NETWORK OPTIMIZATION PROBLEMS[[Link]](https://arxiv.org/pdf/1412.6544.pdf)
- The Loss Surfaces of Multilayer Networks [[Link]](https://arxiv.org/abs/1412.0233)
- Visualizing the Loss Landscape of Neural Nets [[Link]](https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf)
- The Loss Surface Of Deep Linear Networks Viewed Through The Algebraic Geometry Lens [[Link]](https://arxiv.org/pdf/1810.07716.pdf)
- How regularization affects the critical points in linear
networks.[[Link]](http://papers.nips.cc/paper/6844-how-regularization-affects-the-critical-points-in-linear-networks.pdf)
- Local minima in training of neural networks [[Link]](https://arxiv.org/abs/1611.06310)

#### Bifurcations and RNNs difficulty to train
-  Bifurcations of Recurrent Neural Networks in Gradient Descent Learning [[Link]](https://pdfs.semanticscholar.org/b579/27b713a6f9b73c7941f99144165396483478.pdf)
- On the difficulty of training recurrent neural networks [[Link]](http://proceedings.mlr.press/v28/pascanu13.pdf)
- Understanding and Controlling Memory in Recurrent Neural Networks [[Link]](https://arxiv.org/pdf/1902.07275.pdf)
- Dynamics and Bifurcation of Neural Networks [[Link]](https://pdfs.semanticscholar.org/a413/4a36fef5ef55d0ff7dae029d6b8f55140cf7.pdf)
- Context Aware Machine Learning [[Link]](https://arxiv.org/pdf/1901.03415.pdf)
- The trade-off between long-term memory and smoothness for recurrent networks [[Link]](https://arxiv.org/pdf/1906.08482.pdf)
- Dynamical complexity and computation in recurrent neural networks beyond their fxed point [[Link]](https://www.nature.com/articles/s41598-018-21624-2.pdf)
- Bifurcations in discrete-time neural networks : controlling complex network behaviour with inputs [[Links]](https://pub.uni-bielefeld.de/record/2302580)
- Interpreting Recurrent Neural Networks Behaviour via Excitable Network Attractors [[Link]](https://link.springer.com/article/10.1007/s12559-019-09634-2#Sec11)

#### Tutorials and Blogs
- https://www.offconvex.org/ 
- An overview of gradient descent optimization algorithms [[Link]](https://arxiv.org/pdf/1609.04747.pdf)
- Why Momentum really works?[[Link]](https://distill.pub/2017/momentum/)
- Optimization [[Book]](https://www.deeplearningbook.org/contents/optimization.html))

#### Contributing
If you've found any informative resources that you think belong here, be sure to submit a pull request or create an issue! 
