## Content (Working on it!)

#### Popular Optimization algorithms
- SGD [[Book]](https://www.deeplearningbook.org/contents/optimization.html)
- Momentun [[Book]](https://www.deeplearningbook.org/contents/optimization.html)
- RMSProp [[Book]](https://www.deeplearningbook.org/contents/optimization.html)
- AdaGrad [[Link]](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
- ADAM [[Link]](https://arxiv.org/abs/1412.6980)
- AdaBound [[Link]](https://arxiv.org/abs/1902.09843) [[Github]](https://github.com/Luolc/AdaBound)

#### Normalization Methods
- BatchNorm [[Link]](https://arxiv.org/abs/1502.03167)
- Weight Norm [[Link]](http://papers.nips.cc/paper/6113-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks)
- Spectral Norm [[Link]](https://arxiv.org/abs/1802.05957)

#### On Convexity of Neural Networks
- Convex Neural Networks [[Link]](http://papers.nips.cc/paper/2800-convex-neural-networks.pdf)
- Breaking the Curse of Dimensionality with Convex Neural Networks [[Link]](http://jmlr.org/papers/volume18/14-546/14-546.pdf)
- UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION [[Link]](https://arxiv.org/pdf/1611.03530.pdf)
- Optimal Control Via Neural Networks: A Convex Approach. [[Link]](https://openreview.net/forum?id=H1MW72AcK7)
- A New Concept of Convex based Multiple Neural Networks Structure. [[Link](http://www.ifaamas.org/Proceedings/aamas2019/pdfs/p1306.pdf)
- SGD Converges to Global Minimum in Deep Learning via Star-convex Path [[Link]](https://arxiv.org/abs/1901.00451)
- Parameter Continuation with Secant Approximation for Deep Neural Networks [[Thesis]](https://digitalcommons.wpi.edu/etd-theses/1256/)

#### On Loss Surfaces of Deep Neural Networks
- QUALITATIVELY CHARACTERIZING NEURAL NETWORK OPTIMIZATION PROBLEMS[[Link]](https://arxiv.org/pdf/1412.6544.pdf)
- The Loss Surfaces of Multilayer Networks [[Link]](https://arxiv.org/abs/1412.0233)
- Visualizing the Loss Landscape of Neural Nets [[Link]](https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf)
- The Loss Surface Of Deep Linear Networks Viewed Through The Algebraic Geometry Lens [[Link]](https://arxiv.org/pdf/1810.07716.pdf)

#### Tutorials
- An overview of gradient descent optimization algorithms [[Link]](https://arxiv.org/pdf/1609.04747.pdf)
- Why Momentum really works?[[Link]](https://distill.pub/2017/momentum/)
- Optimization ([[Book]](https://www.deeplearningbook.org/contents/optimization.html))
