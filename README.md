## Content (In progress!)

#### Popular Optimization algorithms
- SGD [[Book]](https://www.deeplearningbook.org/contents/optimization.html)
- Momentum [[Book]](https://www.deeplearningbook.org/contents/optimization.html)
- RMSProp [[Book]](https://www.deeplearningbook.org/contents/optimization.html)
- AdaGrad [[Link]](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
- ADAM [[Link]](https://arxiv.org/abs/1412.6980)
- AdaBound [[Link]](https://arxiv.org/abs/1902.09843) [[Github]](https://github.com/Luolc/AdaBound)

#### Normalization Methods
- BatchNorm [[Link]](https://arxiv.org/abs/1502.03167)
- Weight Norm [[Link]](http://papers.nips.cc/paper/6113-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks)
- Spectral Norm [[Link]](https://arxiv.org/abs/1802.05957)

#### On Convexity of Neural Networks
- Convex Neural Networks [[Link]](http://papers.nips.cc/paper/2800-convex-neural-networks.pdf)
- Breaking the Curse of Dimensionality with Convex Neural Networks [[Link]](http://jmlr.org/papers/volume18/14-546/14-546.pdf)
- UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION [[Link]](https://arxiv.org/pdf/1611.03530.pdf)
- Optimal Control Via Neural Networks: A Convex Approach. [[Link]](https://openreview.net/forum?id=H1MW72AcK7)
- Input Convex Neural Networks [[Link]](https://arxiv.org/pdf/1609.07152.pdf)
- A New Concept of Convex based Multiple Neural Networks Structure. [[Link](http://www.ifaamas.org/Proceedings/aamas2019/pdfs/p1306.pdf)
- SGD Converges to Global Minimum in Deep Learning via Star-convex Path [[Link]](https://arxiv.org/abs/1901.00451)

#### Solving simpler problem first (or Activations)
- Curriculum Learning [[Link]](https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf)
- Noisy Activation Function [[Link]](http://proceedings.mlr.press/v48/gulcehre16.pdf)
- Mollifying Networks [[Link]](https://arxiv.org/abs/1608.04980)
- Parameter Continuation with Secant Approximation for Deep Neural Networks and Step-up GAN [[Thesis]](https://digitalcommons.wpi.edu/etd-theses/1256/)
- HashNet: Deep Learning to Hash by Continuation. [[Link]](https://arxiv.org/abs/1702.00758)
- Learning Combinations of Activation Functions. [[Link]](https://arxiv.org/pdf/1801.09403.pdf)

#### On Loss Surfaces of Deep Neural Networks
- QUALITATIVELY CHARACTERIZING NEURAL NETWORK OPTIMIZATION PROBLEMS[[Link]](https://arxiv.org/pdf/1412.6544.pdf)
- The Loss Surfaces of Multilayer Networks [[Link]](https://arxiv.org/abs/1412.0233)
- Visualizing the Loss Landscape of Neural Nets [[Link]](https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf)
- The Loss Surface Of Deep Linear Networks Viewed Through The Algebraic Geometry Lens [[Link]](https://arxiv.org/pdf/1810.07716.pdf)
- How regularization affects the critical points in linear
networks.[[Link]](http://papers.nips.cc/paper/6844-how-regularization-affects-the-critical-points-in-linear-networks.pdf)
- Understanding deep learning requires rethinking generalization. [[Link]](https://openreview.net/forum?id=Sy8gdB9xx)

#### Tutorials
- An overview of gradient descent optimization algorithms [[Link]](https://arxiv.org/pdf/1609.04747.pdf)
- Why Momentum really works?[[Link]](https://distill.pub/2017/momentum/)
- Optimization [[Book]](https://www.deeplearningbook.org/contents/optimization.html))
